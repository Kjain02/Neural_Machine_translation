{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86ddb602",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6049b55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d7eaf9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'unzip' is not recognized as an internal or external command,\",\n",
       " 'operable program or batch file.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!curl -O http://www.manythings.org/anki/fra-eng.zip\n",
    "!!unzip fra-eng.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d73b118",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 100  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "num_samples = 10000  # Number of samples to train on.\n",
    "# Path to the data txt file on disk.\n",
    "data_path = \"fra-eng/fra.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15c7ebec",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.read().split(\"\\n\")\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text, _ = line.split(\"\\t\")\n",
    "    # We use \"tab\" as the \"start sequence\" character\n",
    "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "    target_text = \"\\t\" + target_text + \"\\n\"\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "633cf809",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#     lines = f.read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1fa8db14",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'See you later.\\tOn se voit plus tard.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1841588 (CK) & #1843145 (sacredceltic)'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[8763]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6ebbbc2",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1c79dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 10000\n",
      "Number of unique input tokens: 70\n",
      "Number of unique output tokens: 93\n",
      "Max sequence length for inputs: 14\n",
      "Max sequence length for outputs: 59\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of samples:\", len(input_texts))\n",
    "print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
    "print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
    "print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
    "print(\"Max sequence length for outputs:\", max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89917b57",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01d9ffe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 0,\n",
       " '!': 1,\n",
       " '\"': 2,\n",
       " '$': 3,\n",
       " '%': 4,\n",
       " '&': 5,\n",
       " \"'\": 6,\n",
       " ',': 7,\n",
       " '-': 8,\n",
       " '.': 9,\n",
       " '0': 10,\n",
       " '1': 11,\n",
       " '2': 12,\n",
       " '3': 13,\n",
       " '5': 14,\n",
       " '7': 15,\n",
       " '8': 16,\n",
       " '9': 17,\n",
       " ':': 18,\n",
       " '?': 19,\n",
       " 'A': 20,\n",
       " 'B': 21,\n",
       " 'C': 22,\n",
       " 'D': 23,\n",
       " 'E': 24,\n",
       " 'F': 25,\n",
       " 'G': 26,\n",
       " 'H': 27,\n",
       " 'I': 28,\n",
       " 'J': 29,\n",
       " 'K': 30,\n",
       " 'L': 31,\n",
       " 'M': 32,\n",
       " 'N': 33,\n",
       " 'O': 34,\n",
       " 'P': 35,\n",
       " 'Q': 36,\n",
       " 'R': 37,\n",
       " 'S': 38,\n",
       " 'T': 39,\n",
       " 'U': 40,\n",
       " 'V': 41,\n",
       " 'W': 42,\n",
       " 'Y': 43,\n",
       " 'a': 44,\n",
       " 'b': 45,\n",
       " 'c': 46,\n",
       " 'd': 47,\n",
       " 'e': 48,\n",
       " 'f': 49,\n",
       " 'g': 50,\n",
       " 'h': 51,\n",
       " 'i': 52,\n",
       " 'j': 53,\n",
       " 'k': 54,\n",
       " 'l': 55,\n",
       " 'm': 56,\n",
       " 'n': 57,\n",
       " 'o': 58,\n",
       " 'p': 59,\n",
       " 'q': 60,\n",
       " 'r': 61,\n",
       " 's': 62,\n",
       " 't': 63,\n",
       " 'u': 64,\n",
       " 'v': 65,\n",
       " 'w': 66,\n",
       " 'x': 67,\n",
       " 'y': 68,\n",
       " 'z': 69}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_token_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b30cbb42",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\"\n",
    ")\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
    ")\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f062371f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3448d47b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.0\n",
    "    encoder_input_data[i, t + 1 :, input_token_index[\" \"]] = 1.0\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.0\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
    "    decoder_input_data[i, t + 1 :, target_token_index[\" \"]] = 1.0\n",
    "    decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b3422df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data[8763]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c51ec2af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "53c3bb88",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define an input sequence and process it.\n",
    "encoder_inputs = keras.Input(shape=(None, num_encoder_tokens))\n",
    "encoder = keras.layers.LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c97afe18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<KerasTensor: shape=(None, 256) dtype=float32 (created by layer 'lstm')>,\n",
       " <KerasTensor: shape=(None, 256) dtype=float32 (created by layer 'lstm')>]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0905b4f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 256) dtype=bool (created by layer 'tf.__operators__.eq_2')>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3c028f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = keras.Input(shape=(None, num_decoder_tokens))\n",
    "\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_dense = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\")\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c62bdacc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<KerasTensor: shape=(None, None, 93) dtype=float32 (created by layer 'dense_2')>,\n",
       " <KerasTensor: shape=(None, 256) dtype=float32 (created by layer 'lstm_3')>)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_outputs,_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "df546a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d0df908e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, None, 70)]   0           []                               \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(None, None, 93)]   0           []                               \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    [(None, 256),        334848      ['input_1[0][0]']                \n",
      "                                 (None, 256),                                                     \n",
      "                                 (None, 256)]                                                     \n",
      "                                                                                                  \n",
      " lstm_3 (LSTM)                  [(None, None, 256),  358400      ['input_4[0][0]',                \n",
      "                                 (None, 256),                     'lstm[0][1]',                   \n",
      "                                 (None, 256)]                     'lstm[0][2]']                   \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, None, 93)     23901       ['lstm_3[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 717,149\n",
      "Trainable params: 717,149\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b3bdd9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "125/125 [==============================] - 12s 19ms/step - loss: 1.1504 - accuracy: 0.7375 - val_loss: 1.0819 - val_accuracy: 0.7306\n",
      "Epoch 2/100\n",
      "125/125 [==============================] - 2s 12ms/step - loss: 0.8108 - accuracy: 0.7798 - val_loss: 0.7792 - val_accuracy: 0.7848\n",
      "Epoch 3/100\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.6563 - accuracy: 0.8161 - val_loss: 0.6769 - val_accuracy: 0.8053\n",
      "Epoch 4/100\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.5719 - accuracy: 0.8339 - val_loss: 0.6068 - val_accuracy: 0.8239\n",
      "Epoch 5/100\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.5218 - accuracy: 0.8481 - val_loss: 0.5751 - val_accuracy: 0.8327\n",
      "Epoch 6/100\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.4855 - accuracy: 0.8574 - val_loss: 0.5334 - val_accuracy: 0.8456\n",
      "Epoch 7/100\n",
      "125/125 [==============================] - 2s 12ms/step - loss: 0.4571 - accuracy: 0.8651 - val_loss: 0.5141 - val_accuracy: 0.8495\n",
      "Epoch 8/100\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.4342 - accuracy: 0.8711 - val_loss: 0.5025 - val_accuracy: 0.8518\n",
      "Epoch 9/100\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.4142 - accuracy: 0.8765 - val_loss: 0.4796 - val_accuracy: 0.8581\n",
      "Epoch 10/100\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.3959 - accuracy: 0.8816 - val_loss: 0.4732 - val_accuracy: 0.8594\n",
      "Epoch 11/100\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.3784 - accuracy: 0.8868 - val_loss: 0.4538 - val_accuracy: 0.8646\n",
      "Epoch 12/100\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.3630 - accuracy: 0.8914 - val_loss: 0.4490 - val_accuracy: 0.8667\n",
      "Epoch 13/100\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.3479 - accuracy: 0.8954 - val_loss: 0.4399 - val_accuracy: 0.8694\n",
      "Epoch 14/100\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.3343 - accuracy: 0.8996 - val_loss: 0.4340 - val_accuracy: 0.8718\n",
      "Epoch 15/100\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.3209 - accuracy: 0.9035 - val_loss: 0.4345 - val_accuracy: 0.8721\n",
      "Epoch 16/100\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.3085 - accuracy: 0.9072 - val_loss: 0.4236 - val_accuracy: 0.8755\n",
      "Epoch 17/100\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.2968 - accuracy: 0.9107 - val_loss: 0.4179 - val_accuracy: 0.8775\n",
      "Epoch 18/100\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.2852 - accuracy: 0.9140 - val_loss: 0.4242 - val_accuracy: 0.8771\n",
      "Epoch 19/100\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.2750 - accuracy: 0.9171 - val_loss: 0.4172 - val_accuracy: 0.8798\n",
      "Epoch 20/100\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.2648 - accuracy: 0.9201 - val_loss: 0.4150 - val_accuracy: 0.8801\n",
      "Epoch 21/100\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.2552 - accuracy: 0.9230 - val_loss: 0.4152 - val_accuracy: 0.8807\n",
      "Epoch 22/100\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.2462 - accuracy: 0.9257 - val_loss: 0.4146 - val_accuracy: 0.8811\n",
      "Epoch 23/100\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.2371 - accuracy: 0.9285 - val_loss: 0.4169 - val_accuracy: 0.8826\n",
      "Epoch 24/100\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.2289 - accuracy: 0.9310 - val_loss: 0.4245 - val_accuracy: 0.8814\n",
      "Epoch 25/100\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.2214 - accuracy: 0.9329 - val_loss: 0.4193 - val_accuracy: 0.8827\n",
      "Epoch 26/100\n",
      "125/125 [==============================] - 2s 14ms/step - loss: 0.2138 - accuracy: 0.9354 - val_loss: 0.4175 - val_accuracy: 0.8841\n",
      "Epoch 27/100\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.2063 - accuracy: 0.9374 - val_loss: 0.4242 - val_accuracy: 0.8826\n",
      "Epoch 28/100\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.1997 - accuracy: 0.9394 - val_loss: 0.4236 - val_accuracy: 0.8831\n",
      "Epoch 29/100\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.1929 - accuracy: 0.9412 - val_loss: 0.4273 - val_accuracy: 0.8832\n",
      "Epoch 30/100\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.1868 - accuracy: 0.9431 - val_loss: 0.4282 - val_accuracy: 0.8837\n",
      "Epoch 31/100\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.1807 - accuracy: 0.9450 - val_loss: 0.4301 - val_accuracy: 0.8842\n",
      "Epoch 32/100\n",
      "125/125 [==============================] - 2s 14ms/step - loss: 0.1746 - accuracy: 0.9469 - val_loss: 0.4367 - val_accuracy: 0.8832\n",
      "Epoch 33/100\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.1691 - accuracy: 0.9484 - val_loss: 0.4373 - val_accuracy: 0.8833\n",
      "Epoch 34/100\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.1637 - accuracy: 0.9501 - val_loss: 0.4426 - val_accuracy: 0.8837\n",
      "Epoch 35/100\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.1585 - accuracy: 0.9516 - val_loss: 0.4449 - val_accuracy: 0.8837\n",
      "Epoch 36/100\n",
      "125/125 [==============================] - 2s 14ms/step - loss: 0.1537 - accuracy: 0.9531 - val_loss: 0.4525 - val_accuracy: 0.8834\n",
      "Epoch 37/100\n",
      "125/125 [==============================] - 2s 15ms/step - loss: 0.1490 - accuracy: 0.9544 - val_loss: 0.4540 - val_accuracy: 0.8838\n",
      "Epoch 38/100\n",
      "125/125 [==============================] - 2s 14ms/step - loss: 0.1447 - accuracy: 0.9554 - val_loss: 0.4607 - val_accuracy: 0.8839\n",
      "Epoch 39/100\n",
      "125/125 [==============================] - 2s 14ms/step - loss: 0.1405 - accuracy: 0.9567 - val_loss: 0.4614 - val_accuracy: 0.8845\n",
      "Epoch 40/100\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.1357 - accuracy: 0.9584 - val_loss: 0.4688 - val_accuracy: 0.8838\n",
      "Epoch 41/100\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.1322 - accuracy: 0.9592 - val_loss: 0.4681 - val_accuracy: 0.8829\n",
      "Epoch 42/100\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.1279 - accuracy: 0.9606 - val_loss: 0.4751 - val_accuracy: 0.8829\n",
      "Epoch 43/100\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.1244 - accuracy: 0.9614 - val_loss: 0.4772 - val_accuracy: 0.8830\n",
      "Epoch 44/100\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.1208 - accuracy: 0.9624 - val_loss: 0.4823 - val_accuracy: 0.8833\n",
      "Epoch 45/100\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.1175 - accuracy: 0.9635 - val_loss: 0.4869 - val_accuracy: 0.8835\n",
      "Epoch 46/100\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.1143 - accuracy: 0.9645 - val_loss: 0.4935 - val_accuracy: 0.8818\n",
      "Epoch 47/100\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.1108 - accuracy: 0.9655 - val_loss: 0.4979 - val_accuracy: 0.8822\n",
      "Epoch 48/100\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.1077 - accuracy: 0.9666 - val_loss: 0.5003 - val_accuracy: 0.8827\n",
      "Epoch 49/100\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.1054 - accuracy: 0.9667 - val_loss: 0.5028 - val_accuracy: 0.8828\n",
      "Epoch 50/100\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.1023 - accuracy: 0.9679 - val_loss: 0.5093 - val_accuracy: 0.8823\n",
      "Epoch 51/100\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.0998 - accuracy: 0.9688 - val_loss: 0.5140 - val_accuracy: 0.8825\n",
      "Epoch 52/100\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.0967 - accuracy: 0.9696 - val_loss: 0.5214 - val_accuracy: 0.8818\n",
      "Epoch 53/100\n",
      "125/125 [==============================] - 2s 14ms/step - loss: 0.0942 - accuracy: 0.9702 - val_loss: 0.5222 - val_accuracy: 0.8819\n",
      "Epoch 54/100\n",
      "125/125 [==============================] - 2s 14ms/step - loss: 0.0923 - accuracy: 0.9708 - val_loss: 0.5317 - val_accuracy: 0.8806\n",
      "Epoch 55/100\n",
      "125/125 [==============================] - 2s 15ms/step - loss: 0.0899 - accuracy: 0.9715 - val_loss: 0.5332 - val_accuracy: 0.8826\n",
      "Epoch 56/100\n",
      "125/125 [==============================] - 2s 14ms/step - loss: 0.0874 - accuracy: 0.9724 - val_loss: 0.5385 - val_accuracy: 0.8810\n",
      "Epoch 57/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 2s 13ms/step - loss: 0.0855 - accuracy: 0.9728 - val_loss: 0.5431 - val_accuracy: 0.8815\n",
      "Epoch 58/100\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.0835 - accuracy: 0.9732 - val_loss: 0.5506 - val_accuracy: 0.8812\n",
      "Epoch 59/100\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.0813 - accuracy: 0.9739 - val_loss: 0.5500 - val_accuracy: 0.8810\n",
      "Epoch 60/100\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.0797 - accuracy: 0.9745 - val_loss: 0.5600 - val_accuracy: 0.8812\n",
      "Epoch 61/100\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.0778 - accuracy: 0.9749 - val_loss: 0.5583 - val_accuracy: 0.8817\n",
      "Epoch 62/100\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.0760 - accuracy: 0.9754 - val_loss: 0.5662 - val_accuracy: 0.8813\n",
      "Epoch 63/100\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.0743 - accuracy: 0.9758 - val_loss: 0.5640 - val_accuracy: 0.8820\n",
      "Epoch 64/100\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.0725 - accuracy: 0.9766 - val_loss: 0.5750 - val_accuracy: 0.8812\n",
      "Epoch 65/100\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.0706 - accuracy: 0.9771 - val_loss: 0.5783 - val_accuracy: 0.8812\n",
      "Epoch 66/100\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.0696 - accuracy: 0.9775 - val_loss: 0.5825 - val_accuracy: 0.8811\n",
      "Epoch 67/100\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.0682 - accuracy: 0.9777 - val_loss: 0.5856 - val_accuracy: 0.8818\n",
      "Epoch 68/100\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.0667 - accuracy: 0.9781 - val_loss: 0.5912 - val_accuracy: 0.8813\n",
      "Epoch 69/100\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.0653 - accuracy: 0.9785 - val_loss: 0.5951 - val_accuracy: 0.8805\n",
      "Epoch 70/100\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.0641 - accuracy: 0.9790 - val_loss: 0.5974 - val_accuracy: 0.8813\n",
      "Epoch 71/100\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.0627 - accuracy: 0.9792 - val_loss: 0.6029 - val_accuracy: 0.8809\n",
      "Epoch 72/100\n",
      "125/125 [==============================] - 2s 15ms/step - loss: 0.0615 - accuracy: 0.9795 - val_loss: 0.6018 - val_accuracy: 0.8814\n",
      "Epoch 73/100\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.0606 - accuracy: 0.9796 - val_loss: 0.6174 - val_accuracy: 0.8795\n",
      "Epoch 74/100\n",
      "125/125 [==============================] - 2s 14ms/step - loss: 0.0594 - accuracy: 0.9799 - val_loss: 0.6131 - val_accuracy: 0.8806\n",
      "Epoch 75/100\n",
      "125/125 [==============================] - 2s 14ms/step - loss: 0.0582 - accuracy: 0.9804 - val_loss: 0.6144 - val_accuracy: 0.8809\n",
      "Epoch 76/100\n",
      "125/125 [==============================] - 2s 14ms/step - loss: 0.0571 - accuracy: 0.9808 - val_loss: 0.6219 - val_accuracy: 0.8805\n",
      "Epoch 77/100\n",
      "125/125 [==============================] - 2s 14ms/step - loss: 0.0563 - accuracy: 0.9810 - val_loss: 0.6309 - val_accuracy: 0.8799\n",
      "Epoch 78/100\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.0550 - accuracy: 0.9815 - val_loss: 0.6322 - val_accuracy: 0.8800\n",
      "Epoch 79/100\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.0542 - accuracy: 0.9817 - val_loss: 0.6304 - val_accuracy: 0.8810\n",
      "Epoch 80/100\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.0537 - accuracy: 0.9816 - val_loss: 0.6365 - val_accuracy: 0.8804\n",
      "Epoch 81/100\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.0526 - accuracy: 0.9821 - val_loss: 0.6393 - val_accuracy: 0.8805\n",
      "Epoch 82/100\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.0520 - accuracy: 0.9820 - val_loss: 0.6471 - val_accuracy: 0.8798\n",
      "Epoch 83/100\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.0509 - accuracy: 0.9824 - val_loss: 0.6476 - val_accuracy: 0.8797\n",
      "Epoch 84/100\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.0503 - accuracy: 0.9826 - val_loss: 0.6461 - val_accuracy: 0.8810\n",
      "Epoch 85/100\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.0494 - accuracy: 0.9831 - val_loss: 0.6553 - val_accuracy: 0.8798\n",
      "Epoch 86/100\n",
      "125/125 [==============================] - 2s 14ms/step - loss: 0.0486 - accuracy: 0.9831 - val_loss: 0.6536 - val_accuracy: 0.8807\n",
      "Epoch 87/100\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.0482 - accuracy: 0.9833 - val_loss: 0.6606 - val_accuracy: 0.8799\n",
      "Epoch 88/100\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.0474 - accuracy: 0.9834 - val_loss: 0.6629 - val_accuracy: 0.8802\n",
      "Epoch 89/100\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.0467 - accuracy: 0.9837 - val_loss: 0.6698 - val_accuracy: 0.8798\n",
      "Epoch 90/100\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.0463 - accuracy: 0.9837 - val_loss: 0.6696 - val_accuracy: 0.8798\n",
      "Epoch 91/100\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.0456 - accuracy: 0.9840 - val_loss: 0.6775 - val_accuracy: 0.8793\n",
      "Epoch 92/100\n",
      "125/125 [==============================] - 2s 14ms/step - loss: 0.0453 - accuracy: 0.9839 - val_loss: 0.6745 - val_accuracy: 0.8789\n",
      "Epoch 93/100\n",
      "125/125 [==============================] - 2s 14ms/step - loss: 0.0446 - accuracy: 0.9842 - val_loss: 0.6804 - val_accuracy: 0.8791\n",
      "Epoch 94/100\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.0440 - accuracy: 0.9844 - val_loss: 0.6788 - val_accuracy: 0.8802\n",
      "Epoch 95/100\n",
      "125/125 [==============================] - 2s 14ms/step - loss: 0.0434 - accuracy: 0.9845 - val_loss: 0.6844 - val_accuracy: 0.8797\n",
      "Epoch 96/100\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.0431 - accuracy: 0.9847 - val_loss: 0.6874 - val_accuracy: 0.8792\n",
      "Epoch 97/100\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.0428 - accuracy: 0.9848 - val_loss: 0.6891 - val_accuracy: 0.8798\n",
      "Epoch 98/100\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.0422 - accuracy: 0.9848 - val_loss: 0.6883 - val_accuracy: 0.8797\n",
      "Epoch 99/100\n",
      "125/125 [==============================] - 2s 14ms/step - loss: 0.0418 - accuracy: 0.9848 - val_loss: 0.6981 - val_accuracy: 0.8799\n",
      "Epoch 100/100\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.0412 - accuracy: 0.9851 - val_loss: 0.7011 - val_accuracy: 0.8791\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17c0da0d700>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "model.fit(\n",
    "    [encoder_input_data, decoder_input_data],\n",
    "    decoder_target_data,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f5045259",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: s2s\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: s2s\\assets\n"
     ]
    }
   ],
   "source": [
    "# Save model\n",
    "model.save(\"s2s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "50309390",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"s2s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "863ece28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_inputs = model.input[0]  # input_1\n",
    "encoder_inputs = model.input[0]  # input_1\n",
    "encoder_outputs, state_h_enc, state_c_enc = model.layers[2].output  # lstm_1\n",
    "encoder_states = [state_h_enc, state_c_enc]\n",
    "encoder_model = keras.Model(encoder_inputs, encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "51294d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = model.input[1]  # input_2\n",
    "decoder_state_input_h = keras.Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = keras.Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_lstm = model.layers[3]\n",
    "decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs\n",
    ")\n",
    "decoder_states = [state_h_dec, state_c_dec]\n",
    "decoder_dense = model.layers[4]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = keras.Model(\n",
    "    [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1719465f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "718bfc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index[\"\\t\"]] = 1.0\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.0\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "76e45283",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = np.zeros(\n",
    "    (5,max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "846a32bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t, char in enumerate('Go.'):\n",
    "    test_input[2,t, input_token_index[char]] = 1.0\n",
    "test_input[2,t + 1 :, input_token_index[\" \"]] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ab55178f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c4792687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 7ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'En route !\\n'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ouput = decode_sequence(test_input[2:3])\n",
    "test_ouput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cd2862",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6829738",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c923aefd",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0540ef42",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095d374b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21648cdc",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e743f047",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
